{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c25540e-c2e6-4fb5-a898-3bffdecc1088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://www.foodnetwork.com/recipes/alaskan-sushi-recipe-1911401: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/smoked-salmon-and-caviar-sushi-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/bbq-chicken-pizza-roll-3416306: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/party-sausage-pizza-rolls (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/beef-and-black-bean-tacodiles-3362628: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/steak-and-black-bean-chalupas.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/food-network-kitchen/blt-dogs-recipe-2105201: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /grilling/grilling-central-burgers-and-hot-dogs/easy-hot-dog-topping-ideas (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/bacon-ricotta-and-potato-strata-3416477: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/cheese-strata-with-ham-and-tomatoes-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/bacon-and-goat-cheese-hash-brown-quiche-3415873: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/leek-bacon-and-gruyere-crustless-quiche-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/food-network-kitchen/bacon-wrapped-splitters-recipe-2105064: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /grilling/grilling-central-burgers-and-hot-dogs/easy-hot-dog-topping-ideas (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/food-network-kitchen/california-dogs-recipe-2104979: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /grilling/grilling-central-burgers-and-hot-dogs/easy-hot-dog-topping-ideas (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/california-roll-rice-salad-recipe-2121138: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/grilled-salmon-sushi-rice-bowl-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/cannoli-cups-recipe-2111962: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/low-carb-cannoli-parfaits-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/dark-chocolate-cups-with-avocado-mousse-3416343: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/giada-de-laurentiis/chocolate-avocado-mousse-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/dijonnaise-dynamite-salmon-recipe-2111739: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/mustard-maple-roasted-salmon-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/dirty-banana-recipe-1911293: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/dirty-banana-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/easy-vegetable-frittata-3414923: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/vegetable-frittata-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/egg-tips-breakfast-for-dinner-recipe-2117603: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/photos/easy-breakfast-for-dinner-recipes (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/eggs-benedict-casserole-3414711: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/cheddar-ham-and-egg-casserole-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/fruit-cockatiel-cupcakes-3362230: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/photos/easy-kid-friendly-desserts.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/famous-queso-dip-3362556: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/jalapeno-queso-fundido-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/famous-queso-dip-light-my-fire-3362222: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/queso-fundido-con-chorizo-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/famous-queso-dip-stoplight-style-3362423: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/queso-fundido-con-chorizo-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/farmhouse-frittata-3415756: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/frittata-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/fiesta-shrimp-tacos-3416587: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/shrimp-tacos-with-grapefruit-salad (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/game-winning-7-layer-dip-3415456: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/six-layers-and-a-chip-dip-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/garlicky-pernil-with-shallot-gravy-3414917: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/slow-roasted-pork-with-citrus-and-garlic-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/general-tsos-party-skewers-3415944: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/asian-chicken-skewers-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/bobby-flay/hamburgers-with-double-cheddar-cheese-grilled-vidalia-onions-and-horseradish-mustard-recipe-1938092: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /content/food-com/en/grilling/grilling-central-burgers.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/harvest-morning-muffins-recipe-2268841: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/sunny-anderson/sunny-morning-muffins-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/impossibly-easy-breakfast-bake-recipe-2129561: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/jamie-deen/breakfast-bake-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/impossibly-easy-chicken-pot-pie-recipe-2129546: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/sunny-anderson/easy-chicken-pot-pie-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/impossibly-easy-taco-pie-recipe-2129392: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/taco-pie-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/jamaican-jerk-chicken-recipe-1939760: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/jamaican-jerk-chicken-recipe0 (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/jasmine-rice-pilaf-with-peas-mint-and-lemon-3416089: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/fragrant-jasmine-rice-pilaf-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/food-network-kitchen/jerk-chicken-dogs-recipe-2105074: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /grilling/grilling-central-burgers-and-hot-dogs/easy-hot-dog-topping-ideas (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/kermits-key-lime-pie-recipe-1939815: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/key-lime-pie-recipe0 (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/jamie-oliver/lamb-curry-recipe-1942035: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/tyler-florence/southern-indian-lamb-curry-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/lasagna-pasta-toss-recipe-2126379: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/tyler-florence/angel-hair-pasta-with-pesto-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/lemon-cream-pancake-puffs-recipe-2121001: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/bobby-flay/lemon-ricotta-pancakes-with-lemon-curd-and-fresh-raspberries-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/lemon-meringue-pie-lets-recipe-2111833: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/ina-garten/lemon-meringue-tart-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/mac-and-cheese-jack-o-lantern-recipe-2269260: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/cheeseburger-mac-attack-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/mac-cheese-muffins-recipe-2111753: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/mini-mac-and-shrooms-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/mackerel-rundown-recipe-1911743: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/alexandra-guarnaschelli/roasted-whole-mackerel-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/nabe-yaki-udon-soup-recipe-1939480: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/japanese-mushroom-egg-noodle-soup-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/food-network-kitchen/new-york-street-dogs-recipe-2105212: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /grilling/grilling-central-burgers-and-hot-dogs/easy-hot-dog-topping-ideas (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/nutty-squash-soup-2268714: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/butternut-squash-soup-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/olive-stuffed-skirt-steaks-with-black-bean-sauce-recipe-1922026: HTTPSConnectionPool(host='www.foodnetwork.com', port=443): Max retries exceeded with url: /recipes/olive-stuffed-skirt-steaks-with-black-bean-sauce-recipe-1922026 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001EF927C9450>: Failed to resolve 'www.foodnetwork.com' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/pbj-bars-recipe-2126430: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/ina-garten/peanut-butter-and-jelly-bars-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/pacific-halibut-with-beets-parsnip-silk-and-macadamia-vinaigrette-3362254: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/macadamia-crusted-halibut-oven-roasted-asparagus-spicy-mango-salsa-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/food-network-kitchen/pan-fried-salmon-recipe-1928086: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/pan-fried-salmon-recipe2.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/quick-pumpkin-flan-3416891: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/orange-coffee-flan-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/quickie-honey-chipotle-grilled-wings-3414783: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/mango-wings (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/quinoa-salad-3415120: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/quinoa-salad-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/raviolis-of-maine-lobster-with-white-truffle-sauce-recipe-1915938: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /content/food-com/en/recipes.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/food-network-kitchen/taco-dogs-recipe-2104988: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /grilling/grilling-central-burgers-and-hot-dogs/easy-hot-dog-topping-ideas (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/tacos-al-pastor-recipe2-2121087: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/jeff-mauro/horizontal-tacos-al-pastor-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/tangy-tropical-pork-chops-recipe-2120594: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/pork-chops-with-pineapple-salsa-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/tapas-pasta-salad-recipe-2112477: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/pasta-salad-with-tuna-celery-white-beans-and-olives-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/tequila-shrimp-recipe-1939243: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/tequila-lime-shrimp-skewers-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/teriyaki-chicken-fajitas-recipe-2111913: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/jeff-mauro/teriyaki-chicken-party-sub-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/veggie-ful-potato-salad-recipe-2112548: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/summer-chopped-salad-with-ranch-dressing-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/food-network-kitchen/vidalia-dogs-recipe-2105097: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /grilling/grilling-central-burgers-and-hot-dogs/easy-hot-dog-topping-ideas (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/watermelophant-punch-3362540: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/ingrid-hoffmann/watermelon-juice-recipe.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/jamie-oliver/warm-salad-of-roasted-squash-prosciutto-and-pecorino-recipe-1914530: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/ina-garten/roasted-butternut-squash-salad-with-warm-cider-vinaigrette-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/watermelon-lemonade-slushie-recipe-2126464: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/watermelon-jalapeno-lemonade.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/whiskey-steak-marinade-recipe-1939329: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/trisha-yearwood/karris-steak-marinade-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/white-bean-and-chicken-chili-recipe-2268856: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/food-network-kitchens/white-chicken-chili (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/wood-grilled-sirloin-with-homemade-french-fries-recipe-1915958: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /content/food-com/en/recipes.html (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "Failed to retrieve https://www.foodnetwork.com/recipes/yellowtail-snapper-with-passion-fruit-beurre-blanc-sauce-recipe-1912149: HTTPSConnectionPool(host='www.foodnetwork.com', port=80): Max retries exceeded with url: /recipes/passion-yellowtail-snapper-recipe (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1006)')))\n",
      "          ID                                     Title  \\\n",
      "0          1                 \"16 Bean\" Pasta E Fagioli   \n",
      "1          2                 \"16 Bean\" Pasta e Fagioli   \n",
      "2          3                            \"21\" Apple Pie   \n",
      "3          4  1 S'more for the Road and Kiddie S'mores   \n",
      "4          5                             1-2-3 Lasagna   \n",
      "...      ...                                       ...   \n",
      "18735  18736               Zuppa di Vongole: Clam Soup   \n",
      "18736  18737      Zuppa e Councesze: Soup with Mussels   \n",
      "18737  18738                     Zuzu's Peach Sparkler   \n",
      "18738  18739    Zwetschgen Datchi (Bavarian Plum Cake)   \n",
      "18739  18740                     Zydeco's 5 BBQ Shrimp   \n",
      "\n",
      "                                                     URL         Level  \\\n",
      "0      https://www.foodnetwork.com/recipes/ina-garten...          Easy   \n",
      "1      https://www.foodnetwork.com/recipes/ina-garten...  Intermediate   \n",
      "2      https://www.foodnetwork.com/recipes/21-apple-p...          Easy   \n",
      "3      https://www.foodnetwork.com/recipes/jeff-mauro...          Easy   \n",
      "4      https://www.foodnetwork.com/recipes/1-2-3-lasa...  Intermediate   \n",
      "...                                                  ...           ...   \n",
      "18735  https://www.foodnetwork.com/recipes/zuppa-di-v...  Intermediate   \n",
      "18736  https://www.foodnetwork.com/recipes/zuppa-e-co...          Easy   \n",
      "18737  https://www.foodnetwork.com/recipes/zuzus-peac...                 \n",
      "18738  https://www.foodnetwork.com/recipes/zwetschgen...          Easy   \n",
      "18739  https://www.foodnetwork.com/recipes/zydecos-5-...          Easy   \n",
      "\n",
      "              Total                                        Ingredients  \\\n",
      "0       1 hr 30 min  [1 (1-pound) bag Goya 16 Bean Soup Mix, 2 tabl...   \n",
      "1      10 hr 10 min  [1 (1-pound) bag 16 Bean Soup Mix, 2 tablespoo...   \n",
      "2                    [2 cups all-purpose flour, 1/2 teaspoon salt, ...   \n",
      "3              2 hr  [Nonstick canola oil spray, 1 pound marshmallo...   \n",
      "4       2 hr 15 min  [1 (15-ounce) container ricotta, 1 cup grated ...   \n",
      "...             ...                                                ...   \n",
      "18735        40 min  [2 pounds fresh clams, 1/4 cup extra-virgin ol...   \n",
      "18736        45 min  [1/2 cup/125 ml extra-virgin olive oil, 2 clov...   \n",
      "18737                [1-1/2 oz. cognac, 1/2 oz. peach schnapps, 1/4...   \n",
      "18738                [1 piece fresh yeast, 6 1/4 tablespoons sugar,...   \n",
      "18739        25 min  [1 pound butter, 1/2 cup chopped green onion, ...   \n",
      "\n",
      "                                              Directions  Cuisine Vegetarian?  \n",
      "0      [The day before you plan to make the soup, pla...  Italian          No  \n",
      "1      [The day before you plan to make the soup, pla...  Italian          No  \n",
      "2      [To prepare the dough by hand, combine the flo...                   No  \n",
      "3      [Coat a large microwave-safe bowl with nonstic...                   No  \n",
      "4      [Preheat the oven to 375 degrees F., In a larg...  Italian          No  \n",
      "...                                                  ...      ...         ...  \n",
      "18735  [Remove excess sand from clams by soaking them...  Italian          No  \n",
      "18736  [Heat up 1/4 cup extra-virgin olive oil in a d...  Italian          No  \n",
      "18737  [In a shaker glass add the peach, mint leaves,...                   No  \n",
      "18738  [For the crust: Mix the fresh yeast with 2 tab...                   No  \n",
      "18739  [Take the butter, cut into thirds, and melt in...                   No  \n",
      "\n",
      "[18740 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 2\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/123/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "    \n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df1 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/a/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "    \n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df2 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/b/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "    \n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df3 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/c/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "    \n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df4 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/d/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "    \n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df5 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers excluding exempt pages\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/e/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "    \n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df6 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/f/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "    \n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df7 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/g/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "    \n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df8 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/h/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "    \n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df9 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/i/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "    \n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df10 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/j/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "\n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df11 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 6\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/k/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "\n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df12 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 6\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/l/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "\n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df13 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/m/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "\n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "    \n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df14 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 7\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/n/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "\n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df15 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/o/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "\n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df16 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/p/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "    \n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df17 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 3\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/q/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "\n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df18 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/r/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "\n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df19 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/s/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "\n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df20 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 5\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/t/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "\n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df21 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 2\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/u/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "\n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "\n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df22 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 8\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/v/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "\n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "\n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df23 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 13\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/w/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    " \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "\n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df24 = pd.DataFrame(all_recipes)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Define the range of page numbers\n",
    "start_page = 1\n",
    "end_page = 4\n",
    "\n",
    "# Generate the list of page numbers\n",
    "page_num = [page for page in range(start_page, end_page + 1)]\n",
    "\n",
    "# Initialize an empty list to hold all recipe data\n",
    "all_recipes = []\n",
    "\n",
    "# Initialize the driver outside the loop\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# Navigate to each page and extract data\n",
    "for num in page_num:\n",
    "    driver.get(f'https://www.foodnetwork.com/recipes/recipes-a-z/xyz/p/{num}')\n",
    "    \n",
    "    # Extracting recipe URLs from the main page\n",
    "    base_url = 'https://www.foodnetwork.com'\n",
    "    recipe_urls = []\n",
    "    time.sleep(3)  # Allow the page to load\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    section = soup.find('section', class_='o-Capsule o-SiteIndex')\n",
    "    li_tags = section.find_all('li')\n",
    "    for li in li_tags:\n",
    "        a_tag = li.find('a')\n",
    "        if a_tag and a_tag.get('href'):\n",
    "            full_url = urljoin(base_url, a_tag['href'])\n",
    "            recipe_urls.append(full_url)\n",
    "    \n",
    "    # Extracting detailed information from each recipe page\n",
    "    levels_list = ['Easy', 'Intermediate', 'Advanced']\n",
    "    cuisine_list = ['American', 'Asian', 'Chinese', 'Filipino', 'French', 'Greek', 'Indian', 'Italian', 'Japanese', 'Korean', 'Mediterranean', 'Mexican', 'Middle Eastern', 'Spanish', 'Thai', 'Vietnamese']\n",
    "    \n",
    "    for href in recipe_urls:\n",
    "        # Skip URLs containing ':80'\n",
    "        if ':80' in href:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(href)\n",
    "            response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        except (requests.HTTPError, requests.ConnectionError, requests.exceptions.InvalidURL) as e:\n",
    "            print(f\"Failed to retrieve {href}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            recipe_content = response.content\n",
    "    \n",
    "            # Parse the HTML content with BeautifulSoup\n",
    "            recipe_soup = BeautifulSoup(recipe_content, 'html.parser')\n",
    "    \n",
    "            # Extract specific data (example: page title)\n",
    "            title_element = recipe_soup.find('span', class_='o-AssetTitle__a-HeadlineText')\n",
    "            title = title_element.get_text().strip() if title_element else ' '\n",
    "    \n",
    "            level_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description')\n",
    "            level = level_element.get_text().strip() if level_element and level_element.get_text().strip() in levels_list else ' '\n",
    "    \n",
    "            total_element = recipe_soup.find('span', class_='o-RecipeInfo__a-Description m-RecipeInfo__a-Description--Total')\n",
    "            total = total_element.get_text().strip() if total_element else ' '\n",
    "    \n",
    "            ingredients_section = recipe_soup.find('section', class_='o-Ingredients')\n",
    "            ingredients = []\n",
    "            if ingredients_section:\n",
    "                ingredients_div = ingredients_section.find('div', class_='o-Ingredients__m-Body')\n",
    "                if ingredients_div:\n",
    "                    ingredients_element = ingredients_div.find_all('p', class_='o-Ingredients__a-Ingredient')\n",
    "                    ingredients = [ingredient.get_text().strip() for ingredient in ingredients_element if ingredient.get_text().strip() != 'Deselect All']\n",
    "    \n",
    "            directions_section = recipe_soup.find('section', class_='o-Method')\n",
    "            directions = []\n",
    "            if directions_section:\n",
    "                directions_element = directions_section.find_all('li', class_='o-Method__m-Step')\n",
    "                directions = [direction.get_text().strip() for direction in directions_element]\n",
    "    \n",
    "            cuisine_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            cuisine = ' '\n",
    "            for element in cuisine_element:\n",
    "                cuisine_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if cuisine_text in cuisine_list:\n",
    "                    cuisine = cuisine_text\n",
    "                    break\n",
    "    \n",
    "            diet_type_element = recipe_soup.find_all('a', class_='o-Capsule__a-Tag a-Tag', title=True)\n",
    "            diet_type = 'No'\n",
    "            for element in diet_type_element:\n",
    "                diet_type_text = element['title'].strip()  # Get the 'title' attribute value\n",
    "                if 'Vegetarian' in diet_type_text:\n",
    "                    diet_type = 'Yes'\n",
    "                    break\n",
    "\n",
    "            recipe_data = {\n",
    "                'Title': title,\n",
    "                'URL': href,\n",
    "                'Level': level,\n",
    "                'Total': total,\n",
    "                'Ingredients': ingredients,\n",
    "                'Directions': directions,\n",
    "                'Cuisine': cuisine,\n",
    "                'Vegetarian?': diet_type\n",
    "            }\n",
    "    \n",
    "            all_recipes.append(recipe_data)\n",
    "\n",
    "# Close the driver after processing all pages\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame from the collected recipe data\n",
    "df25 = pd.DataFrame(all_recipes)\n",
    "\n",
    "combined_df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20, df21, df22, df23, df24, df25], ignore_index=True)\n",
    "combined_df.reset_index(inplace=True)\n",
    "combined_df.rename(columns={'index': 'ID'}, inplace=True)\n",
    "combined_df['ID'] = combined_df['ID'] + 1\n",
    "\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7848da99-c8b5-4f89-808f-dadbcdae3501",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(r'C:\\Users\\Shannen\\OneDrive\\Desktop\\SOPHOMORE\\INTERCESSION\\recipes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
